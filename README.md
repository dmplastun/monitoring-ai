# Server Monitoring + AI Log Analyzer with Ollama ğŸ¤–ğŸ“Š

> **Project:** Automated server monitoring + AI-powered log and metrics analysis using [Ollama](https://ollama.com/) 

This project combines traditional infrastructure monitoring (via Prometheus) with AI-based insights powered by local LLMs (Large Language Models), providing not just raw metrics, but also **intelligent analysis**, **problem detection**, and **actionable recommendations**.

Perfect for DevOps engineers, SRE teams, and anyone looking to bring AIOps capabilities into their infrastructure stack.

---

## ğŸ§  How It Works

1. The system collects CPU and RAM usage from Prometheus.
2. These metrics are sent to a local LLM via the Ollama API.
3. The model returns:
   - A criticality score (0â€“100)
   - List of potential issues
   - Recommended actions
4. Results are exposed as Prometheus metrics and logs for alerting and visualization.

---

## âœ… Key Features

- ğŸ” Real-time CPU & RAM metric collection via Prometheus  
- ğŸ’¡ AI-driven system health analysis using Ollama models (`gemma`, `llama3`, `phi`, etc.)  
- ğŸ“Š Exported Prometheus metrics for integration with Grafana or Alertmanager  
- ğŸ“¦ Containerized with Docker & ready for Kubernetes  
- ğŸš€ Lightweight and easy to extend  

---

## ğŸ§© Architecture Overvie

|  Servers / Node  | ----> |   Prometheus   | ----> |   AI Analyzer (Ollama API)  | ----> |   Grafana Dashboard     |




---

## ğŸ› ï¸ Technologies Used

- **AI Analysis**: Python + FastAPI + Ollama
- **Monitoring**: Prometheus
- **Visualization**: Grafana
- **Orchestration**: Kubernetes
- **Containerization**: Docker

---

## ğŸš€ Quick Start (Kubernetes)

### 1. Deploy on Kubernetes

```bash
kubectl apply -f k8s/
```
Includes: 

    ai-analyzer.yaml â€“ main AI analysis service
    ollama-deployment.yaml â€“ Ollama backend
    stress-deployment.yaml â€“ optional stress test pod
    alert-rules.yaml â€“ Prometheus alert rules
     

2. Import Grafana Dashboard 

Open Grafana at:   
http://grafana-url/dashboard/import

Import the dashboard file:  
dashboards/ai-server-monitoring.json


ğŸ§ª Example AI Output
{
  "score": 75,
  "problems": ["High CPU load", "RAM near critical usage"],
  "solutions": [
    "Consider scaling infrastructure",
    "Check background processes",
    "Increase container resources"
  ]
}

ğŸ“ˆ Exported Metrics 

    ai_analysis_score â€“ Overall criticality score (0â€“100)
    ai_reported_cpu_usage â€“ CPU usage (%)
    ai_reported_ram_usage â€“ RAM usage (%)
    ai_service_state â€“ Service state enum: ok/warning/critical
     

ğŸ§° Extensibility 

You can: 

    Add your own LLMs via Ollama
    Extend Prometheus queries
    Integrate with Slack/Telegram for alerts
    Customize prompts for domain-specific scenarios
     

ğŸ” Security 

    All connections run securely within the Kubernetes cluster
    Supports environment variables for secure configuration
    Can be extended with RBAC and TLS for production use
     

ğŸ“ License 

MIT License â€“ free to use and modify. 
ğŸ“¬ Contact 

If you have questions, suggestions, or want to contribute, feel free to open an issue or reach out directly: 

ğŸ“§ dmitrij.plastun@gmail.com 
 
ğŸš§ Roadmap 

    Telegram/Slack notification bot  
    OpenTelemetry support  
    Predictive failure detection module  
    Web UI for prompt/model management  
    Loki-based log analysis extension
 

If you like this project, please give it a â­ï¸ on GitHub. It helps attract contributors and users. 

ğŸ’¡ Tip:  Try different Ollama models (llama3, mistral, phi, dolphin-mixtral) for better results. You can fine-tune prompts in ai-analyzer/app.py. 
